{"nbformat":4,"nbformat_minor":0,"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.3"},"colab":{"name":"rnn_lowlevel.ipynb","provenance":[]}},"cells":[{"cell_type":"code","metadata":{"id":"b2xUKVLtfIp_","colab_type":"code","colab":{}},"source":["# disregard this cell\n","#!nvidia-smi\n","#import os\n","\n","#os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"2\""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"mwIzeJTjfIqM","colab_type":"code","colab":{}},"source":["# add stuff here to make it work in colab..."],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"LmmG1S2efIqU","colab_type":"code","colab":{}},"source":["!python3 prepare_data.py shakespeare_input.txt shake"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"orO7eWYPfIqc","colab_type":"code","colab":{},"outputId":"8b53f87c-7577-4946-cce9-054fa303cf31"},"source":["import pickle\n","\n","import tensorflow as tf\n","from prepare_data import parse_seq\n","\n","\n","# data\n","bs = 128\n","seq_len = 200\n","data = tf.data.TFRecordDataset(\"shake.tfrecords\")\n","data = data.map(lambda x: parse_seq(x, seq_len))\n","data = data.shuffle(46000).batch(bs).repeat()\n","\n","vocab = pickle.load(open(\"shake_vocab\", mode=\"rb\"))\n","vocab_size = len(vocab)\n","ind_to_ch = {ind: ch for (ch, ind) in vocab.items()}\n","\n","print(vocab_size)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["68\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"5xP4U22hfIqk","colab_type":"code","colab":{},"outputId":"5173b68f-cff4-485b-d4a9-4a489dcd1532"},"source":["# don't run this cell lol\n","# it prints all of the data forever\n","for ind, thing in enumerate(data):\n","    inds = thing[0].numpy()\n","    to_chars = \"\".join([ind_to_ch[ind] for ind in inds])\n","    #print(ind)\n","    print(to_chars)\n","    #print()\n","    break"],"execution_count":null,"outputs":[{"output_type":"stream","text":["<S>ell, what worst?\n","\n","Messenger:\n","The nature of bad news infects the teller.\n","\n","MARK ANTONY:\n","When it concerns the fool or coward. On:\n","Things that are past are done with me. 'Tis thus:\n","Who tells me true, tho\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"0zFrz0BFfIqq","colab_type":"code","colab":{}},"source":["# model\n","n_h = 512\n","w_xh = tf.Variable(tf.initializers.glorot_uniform()([vocab_size, n_h]))\n","w_hh = tf.Variable(tf.initializers.glorot_uniform()([n_h, n_h]))\n","b_h = tf.Variable(tf.zeros([n_h]))\n","\n","w_ho = tf.Variable(tf.initializers.glorot_uniform()([n_h, vocab_size]))\n","b_o = tf.Variable(tf.zeros([vocab_size]))\n","\n","all_vars = [w_xh, w_hh, b_h, w_ho, b_o]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"9Px56y7jfIqy","colab_type":"code","colab":{}},"source":["# training\n","# somewhat arbitrary number of steps\n","steps = 20*35000 // bs\n","opt = tf.optimizers.Adam()\n","loss_fn = tf.losses.SparseCategoricalCrossentropy(from_logits=True)\n","\n","\n","@tf.function\n","def run_rnn_on_seq(seq_batch):\n","    with tf.GradientTape() as tape:\n","        state = tf.zeros([tf.shape(seq_batch)[0], n_h])\n","        total_loss = tf.constant(0.)\n","\n","        for time_step in tf.range(tf.shape(seq_batch)[1] - 1):\n","            inp_here = tf.one_hot(seq_batch[:, time_step], vocab_size)\n","            state = tf.nn.tanh(tf.matmul(inp_here, w_xh) + tf.matmul(state, w_hh) + b_h)\n","            logits = tf.matmul(state, w_ho) + b_o\n","\n","            loss_here = loss_fn(seq_batch[:, time_step+1], logits)\n","            total_loss += loss_here\n","            \n","        total_loss /= tf.cast(tf.shape(seq_batch)[1] - 1, tf.float32)\n","    grads = tape.gradient(total_loss, all_vars)\n","    \n","    # this is gradient clipping\n","    glob_norm = tf.linalg.global_norm(grads)\n","    grads = [g/glob_norm for g in grads]\n","    \n","    opt.apply_gradients(zip(grads, all_vars))\n","\n","    return total_loss\n","\n","\n","# alternative function that, instead of summing up the loss at each time step,\n","# builds a \"loss sequence\" over time\n","# in principle, we could just build a list with one element per time step\n","# but this will not work with tf.function (tensors and python lists don't play\n","# together very well) so we use a thing called TensorArray\n","@tf.function\n","def run_rnn_on_seq_alternative(seq_batch):\n","    with tf.GradientTape() as tape:\n","        state = tf.zeros([tf.shape(seq_batch)[0], n_h])\n","        # this is where the per-time step losses will go\n","        losses = tf.TensorArray(tf.float32, size=tf.shape(seq_batch)[1]-1)\n","\n","        for time_step in tf.range(tf.shape(seq_batch)[1] - 1):\n","            inp_here = tf.one_hot(seq_batch[:, time_step], vocab_size)  # batch x vocab\n","            state = tf.nn.tanh(tf.matmul(inp_here, w_xh) + tf.matmul(state, w_hh) + b_h)\n","            logits = tf.matmul(state, w_ho) + b_o\n","\n","            # batch-size loss tensor for this time step\n","            # could still use loss_fn here as in the function above, but that would average over the\n","            # batch already. I would like to keep the batch axis here to show how this could\n","            # be used with a mask (see below). that's why this uses tf.nn.sparse...\n","            loss_here = tf.nn.sparse_softmax_cross_entropy_with_logits(seq_batch[:, time_step+1], logits)\n","            \n","            losses = losses.write(time_step, loss_here)\n","        losses = losses.stack() # put them together in a tensor, but it will be time x batch\n","        losses = tf.transpose(losses, [1, 0]) # not really necessary, but transpose to batch x time\n","        \n","        # if, say, we had a batch x time mask tensor, we could multiply it with the loss here...\n","        #losses = losses * mask\n","        \n","        total_loss = tf.reduce_mean(losses) # average over batch and time axes\n","            \n","        \n","    grads = tape.gradient(total_loss, all_vars)\n","    glob_norm = tf.linalg.global_norm(grads)\n","    grads = [g/glob_norm for g in grads]\n","    opt.apply_gradients(zip(grads, all_vars))\n","\n","    return total_loss"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"scrolled":false,"id":"wY8Cb5GdfIq6","colab_type":"code","colab":{}},"source":["for step, seqs in enumerate(data):\n","    xent_avg = run_rnn_on_seq(seqs)\n","\n","    if not step % 200:\n","        print(\"Step: {} Loss: {}\".format(step, xent_avg))\n","        print()\n","        \n","\n","    if step > steps:\n","        break"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"stG3UGA8fIrB","colab_type":"code","colab":{}},"source":["import numpy as np\n","\n","def sample(n_steps):\n","    state = tf.zeros([1, n_h])\n","    gen = [0]\n","\n","    for step in range(n_steps):\n","        state = tf.nn.tanh(tf.matmul(tf.one_hot(gen[-1:], depth=vocab_size), w_xh) + tf.matmul(state, w_hh) + b_h)\n","        probs = tf.nn.softmax(tf.matmul(state, w_ho) + b_o).numpy()[0]\n","        #gen.append(np.argmax(probs))  # use argmax instead of choice if you want\n","        gen.append(np.random.choice(vocab_size, p=probs))\n","    return \"\".join([ind_to_ch[ind] for ind in gen])\n","        \n","agg = sample(2000)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"9Z0T8A9OfIrG","colab_type":"code","colab":{}},"source":["print(agg)"],"execution_count":null,"outputs":[]}]}